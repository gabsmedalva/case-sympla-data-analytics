{
  "cells": [
    {
      "cell_type": "code",
      "id": "HHvV9oghHAxdRoDBj3MRfmIe",
      "metadata": {
        "tags": [],
        "id": "HHvV9oghHAxdRoDBj3MRfmIe"
      },
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. PARÂMETROS E CONFIGURAÇÕES\n",
        "project_id = \"site-da-laica\"\n",
        "bucket_raw_path = \"gs://sympla/raw/Base de Vendas para Teste Técnico Dados__20250904 (1).csv\"\n",
        "\n",
        "# Nomenclatura de destino na camada Trusted\n",
        "dataset_id = \"trusted\"\n",
        "table_id = \"vendas_historico\"\n",
        "destination_table = f\"{project_id}.{dataset_id}.{table_id}\"\n",
        "bucket_trusted_path = \"gs://sympla/trusted/vendas_historico.parquet\"\n",
        "path_refined_base = \"gs://sympla/refined/\""
      ],
      "metadata": {
        "id": "PJvLDXE_4Q-M"
      },
      "id": "PJvLDXE_4Q-M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. EXTRAÇÃO (EXTRACT - RAW)\n",
        "print(f\"Lendo dados da camada Raw: {bucket_raw_path}\")\n",
        "\n",
        "# Lendo o CSV. O delimitador da base é o ponto e vírgula (;)\n",
        "df = pd.read_csv(bucket_raw_path, sep=';')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "071vU57f4W0a",
        "outputId": "b1498f8a-b0bd-4559-bc7f-41d1d98ed7c4"
      },
      "id": "071vU57f4W0a",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lendo dados da camada Raw: gs://sympla/raw/Base de Vendas para Teste Técnico Dados__20250904 (1).csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from decimal import Decimal, ROUND_HALF_UP\n",
        "# 3. TRANSFORMAÇÃO E NORMALIZAÇÃO (TRANSFORM - TRUSTED)\n",
        "print(\"Iniciando normalização de tipagem de dados...\")\n",
        "\n",
        "# a) Normalização de Data (Mantendo o tipo datetime mas zerando as horas)\n",
        "# O normalize() garante que 2034-02-01 14:30:00 vire 2034-02-01 00:00:00\n",
        "df['dt_venda'] = pd.to_datetime(df['dt_venda'], errors='coerce').dt.normalize()\n",
        "\n",
        "# Remove as linhas nulas\n",
        "df = df.dropna(subset=['dt_venda'])\n",
        "\n",
        "# b) Normalização de Valores Financeiros (Decimal exato com 2 casas)\n",
        "cols_to_decimal = [\n",
        "    'vr_venda',\n",
        "    'vr_medio_ingresso'\n",
        "]\n",
        "\n",
        "for col in cols_to_decimal:\n",
        "    df[col] = (df[col].astype(str)\n",
        "                      .str.replace('.', '', regex=False) # Remove o separador de milhar\n",
        "                      .str.replace(',', '.', regex=False) # Troca a vírgula decimal por ponto\n",
        "                      # Aplica a tipagem Decimal arredondando para 2 casas com precisão matemática\n",
        "                      .apply(lambda x: Decimal(x).quantize(Decimal('0.00'), rounding=ROUND_HALF_UP)))\n",
        "\n",
        "# c) Normalização de Quantidades (Arredondamento para Inteiro)\n",
        "# Algumas colunas possuem vírgula (ex: \"2,5\"). Primeiro passamos para float, arredondamos e viramos int.\n",
        "cols_to_int = [\n",
        "    'qt_ingresso',\n",
        "    'qt_evento',\n",
        "    'qt_produtor',\n",
        "    'qt_ingresso_por_evento',\n",
        "    'qt_evento_por_produtor'\n",
        "]\n",
        "\n",
        "for col in cols_to_int:\n",
        "    df[col] = (df[col].astype(str)\n",
        "                      .str.replace('.', '', regex=False)\n",
        "                      .str.replace(',', '.', regex=False)\n",
        "                      .astype(float) # Força ser número primeiro\n",
        "                      .round(0)      # Arredonda (ex: 2.5 vira 3.0, 2.4 vira 2.0)\n",
        "                      .fillna(0)     # Trata os nulos\n",
        "                      .astype(int))  # Converte definitivamente para Inteiro\n",
        "\n",
        "# d) Normalização de Strings/Dimensões (Caixa Alta e sem espaços inúteis)\n",
        "cols_to_string = [\n",
        "    'nm_localidade_estado',\n",
        "    'tp_produtor_canal_aquisicao',\n",
        "    'tp_tamanho_produtor',\n",
        "    'nm_evento_classificacao_negocio'\n",
        "]\n",
        "\n",
        "for col in cols_to_string:\n",
        "    df[col] = df[col].astype(str).str.strip().str.upper()\n",
        "\n",
        "print(\"Transformações concluídas. Schema atualizado:\")\n",
        "print(df.dtypes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3yLZMvL4keT",
        "outputId": "03200310-95b6-495e-f030-7d48fb30f4f1"
      },
      "id": "W3yLZMvL4keT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando normalização de tipagem de dados...\n",
            "Transformações concluídas. Schema atualizado:\n",
            "dt_venda                           datetime64[ns]\n",
            "nm_localidade_estado                       object\n",
            "tp_produtor_canal_aquisicao                object\n",
            "tp_tamanho_produtor                        object\n",
            "nm_evento_classificacao_negocio            object\n",
            "vr_venda                                   object\n",
            "qt_ingresso                                 int64\n",
            "qt_evento                                   int64\n",
            "qt_produtor                                 int64\n",
            "qt_ingresso_por_evento                      int64\n",
            "qt_evento_por_produtor                      int64\n",
            "vr_medio_ingresso                          object\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. CARGA NO STORAGE (LOAD - TRUSTED)\n",
        "print(f\"Salvando arquivo Parquet normalizado na camada Trusted: {bucket_trusted_path}...\")\n",
        "\n",
        "# Salva diretamente no GCS. O parâmetro index=False evita criar uma coluna extra inútil\n",
        "# A engine 'pyarrow' é o padrão da indústria para lidar com Parquet no Python\n",
        "df.to_parquet(bucket_trusted_path, index=False, engine='pyarrow')\n",
        "\n",
        "print(\"Sucesso! Arquivo Parquet gerado e salvo na camada Trusted do Storage.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWKQLj4n5IrE",
        "outputId": "027edde9-b431-4779-8cfe-ad36a98e57c3"
      },
      "id": "IWKQLj4n5IrE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Salvando arquivo Parquet normalizado na camada Trusted: gs://sympla/trusted/vendas_historico.parquet...\n",
            "Sucesso! Arquivo Parquet gerado e salvo na camada Trusted do Storage.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. CRIAÇÃO DAS TABELAS DIMENSÃO\n",
        "print(f\"Lendo dados da camada Trusted: {bucket_trusted_path}...\")\n",
        "df_trusted = pd.read_parquet(bucket_trusted_path)\n",
        "print(\"Modelando as Tabelas Dimensão...\")\n",
        "\n",
        "# --- DIM TEMPO (Calendário Completo até Dez/2034) ---\n",
        "# Em vez de pegar apenas as datas existentes, geramos um intervalo contínuo\n",
        "data_inicio = df_trusted['dt_venda'].min()\n",
        "data_fim = pd.Timestamp('2034-12-31')\n",
        "\n",
        "# Criamos a sequência diária de datas\n",
        "calendario = pd.date_range(start=data_inicio, end=data_fim, freq='D')\n",
        "df_tempo = pd.DataFrame({'dt_venda': calendario})\n",
        "\n",
        "# Cria as SKs e colunas auxiliares (mantendo sua lógica original)\n",
        "df_tempo['sk_tempo'] = df_tempo['dt_venda'].dt.strftime('%Y%m%d').astype(int)\n",
        "df_tempo['ano'] = df_tempo['dt_venda'].dt.year\n",
        "df_tempo['mes'] = df_tempo['dt_venda'].dt.month\n",
        "df_tempo['trimestre'] = df_tempo['dt_venda'].dt.quarter\n",
        "df_tempo['ano_mes'] = df_tempo['dt_venda'].dt.strftime('%Y-%m')\n",
        "\n",
        "# Converte a coluna original para 'date' (sem horas e sem fuso) logo antes de exportar\n",
        "df_tempo['dt_venda'] = df_tempo['dt_venda'].dt.date\n",
        "\n",
        "# --- DIM LOCALIDADE ---\n",
        "# Extrai estados únicos e gera um ID sequencial\n",
        "df_localidade = df_trusted[['nm_localidade_estado']].drop_duplicates().reset_index(drop=True)\n",
        "df_localidade['sk_localidade'] = df_localidade.index + 1\n",
        "\n",
        "# --- DIM PRODUTOR ---\n",
        "# Combinação única de tamanho e canal de aquisição\n",
        "df_produtor = df_trusted[['tp_tamanho_produtor', 'tp_produtor_canal_aquisicao']].drop_duplicates().reset_index(drop=True)\n",
        "df_produtor['sk_produtor'] = df_produtor.index + 1\n",
        "\n",
        "# --- DIM EVENTO ---\n",
        "# Classificação do negócio (ex: Corporativo, Esportivo)\n",
        "df_evento = df_trusted[['nm_evento_classificacao_negocio']].drop_duplicates().reset_index(drop=True)\n",
        "df_evento['sk_evento'] = df_evento.index + 1\n",
        "\n",
        "print(f\"Dimensões modeladas com sucesso! Calendário expandido até {data_fim.date()}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yN7qneWtUo3h",
        "outputId": "bd01cf25-4f3e-4923-c792-61a9d15437ba"
      },
      "id": "yN7qneWtUo3h",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lendo dados da camada Trusted: gs://sympla/trusted/vendas_historico.parquet...\n",
            "Modelando as Tabelas Dimensão...\n",
            "Dimensões modeladas com sucesso! Calendário expandido até 2034-12-31.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. CRIAÇÃO DA TABELA FATO\n",
        "print(\"Construindo a Tabela Fato (Cruzamento de Chaves)...\")\n",
        "\n",
        "# Fazemos o MERGE (JOIN) do dataframe original com as dimensões para capturar as SKs\n",
        "df_fato = df_trusted.copy()\n",
        "\n",
        "# Join com Tempo\n",
        "df_fato['sk_tempo'] = df_fato['dt_venda'].dt.strftime('%Y%m%d').astype(int)\n",
        "\n",
        "# Join com Localidade\n",
        "df_fato = df_fato.merge(df_localidade, on='nm_localidade_estado', how='left')\n",
        "\n",
        "# Join com Produtor\n",
        "df_fato = df_fato.merge(df_produtor, on=['tp_tamanho_produtor', 'tp_produtor_canal_aquisicao'], how='left')\n",
        "\n",
        "# Join com Evento\n",
        "df_fato = df_fato.merge(df_evento, on='nm_evento_classificacao_negocio', how='left')\n",
        "\n",
        "# Selecionar APENAS as chaves (SKs) e as métricas para a tabela Fato\n",
        "colunas_fato = [\n",
        "    'sk_tempo', 'sk_localidade', 'sk_produtor', 'sk_evento',\n",
        "    'vr_venda', 'qt_ingresso', 'qt_evento', 'qt_produtor',\n",
        "    'qt_ingresso_por_evento', 'qt_evento_por_produtor', 'vr_medio_ingresso'\n",
        "]\n",
        "df_fato = df_fato[colunas_fato]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMatBgAWVAQz",
        "outputId": "8b0baa29-1858-499a-f1e7-fe3f31d3ac3e"
      },
      "id": "yMatBgAWVAQz",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Construindo a Tabela Fato (Cruzamento de Chaves)...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. CARGA NO STORAGE (SALVANDO EM PARQUET)\n",
        "print(\"Salvando o Star Schema na camada Refined (Storage)...\")\n",
        "\n",
        "tabelas = {\n",
        "    \"dim_tempo\": df_tempo,\n",
        "    \"dim_localidade\": df_localidade,\n",
        "    \"dim_produtor\": df_produtor,\n",
        "    \"dim_evento\": df_evento,\n",
        "    \"fato_vendas_case\": df_fato\n",
        "}\n",
        "\n",
        "for nome_tabela, df in tabelas.items():\n",
        "    caminho_destino = f\"{path_refined_base}{nome_tabela}.parquet\"\n",
        "    df.to_parquet(caminho_destino, index=False, engine='pyarrow')\n",
        "    print(f\" -> Salvo: {caminho_destino} | Linhas: {len(df)}\")\n",
        "\n",
        "print(\"\\nModelagem Dimensional (Star Schema) concluída com sucesso!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxT0EIJmVB6f",
        "outputId": "71fbdead-548c-4735-b843-085ff1b9af74"
      },
      "id": "MxT0EIJmVB6f",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Salvando o Star Schema na camada Refined (Storage)...\n",
            " -> Salvo: gs://sympla/refined/dim_tempo.parquet | Linhas: 699\n",
            " -> Salvo: gs://sympla/refined/dim_localidade.parquet | Linhas: 29\n",
            " -> Salvo: gs://sympla/refined/dim_produtor.parquet | Linhas: 10\n",
            " -> Salvo: gs://sympla/refined/dim_evento.parquet | Linhas: 6\n",
            " -> Salvo: gs://sympla/refined/fato_vendas_case.parquet | Linhas: 15004\n",
            "\n",
            "Modelagem Dimensional (Star Schema) concluída com sucesso!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery\n",
        "\n",
        "# 8. PARÂMETROS E CONFIGURAÇÕES\n",
        "# O seu Dataset no BigQuery\n",
        "dataset_id = \"site-da-laica.sympla\"\n",
        "\n",
        "# O caminho base Refined no Storage\n",
        "gcs_base_uri = \"gs://sympla/refined/\"\n",
        "\n",
        "# A lista das tabelas do Star Schema\n",
        "tabelas_star_schema = [\n",
        "    \"dim_tempo\",\n",
        "    \"dim_localidade\",\n",
        "    \"dim_produtor\",\n",
        "    \"dim_evento\",\n",
        "    \"fato_vendas_case\"\n",
        "]\n",
        "\n",
        "# Inicializa o Client do BigQuery\n",
        "client = bigquery.Client(project=\"site-da-laica\")\n",
        "\n",
        "print(f\"Iniciando a carga de dados para Tabelas NATIVAS no dataset: {dataset_id}...\\n\")\n",
        "\n",
        "# LOOP DE CRIAÇÃO E CARGA DAS TABELAS\n",
        "for nome_tabela in tabelas_star_schema:\n",
        "\n",
        "    # Monta a URI exata do arquivo Parquet no Storage\n",
        "    source_uri = f\"{gcs_base_uri}{nome_tabela}.parquet\"\n",
        "\n",
        "    # Define o ID completo da tabela no BigQuery\n",
        "    table_id = f\"{dataset_id}.{nome_tabela}\"\n",
        "\n",
        "    # Configura o Job de Carga (Load Job)\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        source_format=bigquery.SourceFormat.PARQUET,\n",
        "        # WRITE_TRUNCATE substitui os dados antigos pelos novos.\n",
        "        # Ideal para recargas completas de dimensões e fatos.\n",
        "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
        "    )\n",
        "\n",
        "    print(f\"Carregando {source_uri} na tabela {table_id}...\")\n",
        "\n",
        "    # EXECUÇÃO NO BIGQUERY\n",
        "    try:\n",
        "        # Inicia o job de ingestão (copiando do Storage para o disco nativo do BQ)\n",
        "        load_job = client.load_table_from_uri(\n",
        "            source_uri, table_id, job_config=job_config\n",
        "        )\n",
        "\n",
        "        # Aguarda o processamento do job finalizar\n",
        "        load_job.result()\n",
        "\n",
        "        # Verifica a tabela final para confirmar o sucesso\n",
        "        destination_table = client.get_table(table_id)\n",
        "        print(f\"Sucesso! Tabela Nativa criada com {destination_table.num_rows} linhas.\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao carregar a tabela {nome_tabela}: {e}\")\n",
        "\n",
        "print(\"Todas as Tabelas Nativas foram carregadas com sucesso!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJmhUDfsVrlL",
        "outputId": "f60cf02a-4e0b-425e-b3be-e35ca757d016"
      },
      "id": "zJmhUDfsVrlL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando a carga de dados para Tabelas NATIVAS no dataset: site-da-laica.sympla...\n",
            "\n",
            "Carregando gs://sympla/refined/dim_tempo.parquet na tabela site-da-laica.sympla.dim_tempo...\n",
            "Sucesso! Tabela Nativa criada com 699 linhas.\n",
            "\n",
            "Todas as Tabelas Nativas foram carregadas com sucesso!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Modelo de Forecasting (Séries Temporais) para projetar o restante de 2034.\n",
        "query_treino = f\"\"\"\n",
        "SELECT\n",
        "    t.ano,\n",
        "    t.mes,\n",
        "    t.trimestre,\n",
        "    l.sk_localidade,\n",
        "    p.sk_produtor,\n",
        "    e.sk_evento,\n",
        "    SUM(f.vr_venda) as vr_venda,\n",
        "    SUM(f.qt_ingresso) as qt_ingresso,\n",
        "    SUM(f.qt_evento) as qt_evento,\n",
        "    SUM(f.qt_produtor) as qt_produtor\n",
        "FROM `{dataset_id}.fato_vendas` f\n",
        "JOIN `{dataset_id}.dim_tempo` t ON f.sk_tempo = t.sk_tempo\n",
        "JOIN `{dataset_id}.dim_localidade` l ON f.sk_localidade = l.sk_localidade\n",
        "JOIN `{dataset_id}.dim_produtor` p ON f.sk_produtor = p.sk_produtor\n",
        "JOIN `{dataset_id}.dim_evento` e ON f.sk_evento = e.sk_evento\n",
        "WHERE t.dt_venda < '2034-03-01'\n",
        "GROUP BY 1, 2, 3, 4, 5, 6\n",
        "\"\"\"\n",
        "\n",
        "df_train = client.query(query_treino).to_dataframe()\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "import itertools\n",
        "from decimal import Decimal, ROUND_HALF_UP\n",
        "\n",
        "# 10. TREINAMENTO E PREDIÇÃO COM GRANULARIDADE TOTAL\n",
        "targets = ['vr_venda', 'qt_ingresso', 'qt_evento', 'qt_produtor']\n",
        "X = df_train[['ano', 'mes', 'trimestre', 'sk_localidade', 'sk_produtor', 'sk_evento']]\n",
        "y = df_train[targets]\n",
        "\n",
        "# Treino\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "modelo_fato = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "modelo_fato.fit(X_train, y_train)\n",
        "print(f\"Modelo treinado! Score R²: {modelo_fato.score(X_test, y_test):.2f}\")\n",
        "\n",
        "# --- A. GERAR COMBINAÇÕES PARA 2034 (Granularidade Real) ---\n",
        "print(\"Gerando combinações de dimensões para o futuro...\")\n",
        "# Extraímos todas as combinações únicas de dimensões que existiram no histórico\n",
        "dimensoes_unicas = df_train[['sk_localidade', 'sk_produtor', 'sk_evento']].drop_duplicates()\n",
        "\n",
        "# Definimos os meses de Março a Dezembro\n",
        "meses_futuros = pd.DataFrame({'mes': list(range(3, 13))})\n",
        "meses_futuros['ano'] = 2034\n",
        "meses_futuros['trimestre'] = meses_futuros['mes'].apply(lambda m: (m-1)//3 + 1)\n",
        "\n",
        "# CROSS JOIN: Meses x (Localidade/Produtor/Evento) para ter milhares de linhas\n",
        "meses_faltantes = meses_futuros.merge(dimensoes_unicas, how='cross')\n",
        "\n",
        "# --- B. REALIZAR PREDIÇÃO ---\n",
        "X_futuro = meses_faltantes[['ano', 'mes', 'trimestre', 'sk_localidade', 'sk_produtor', 'sk_evento']]\n",
        "previsoes_array = modelo_fato.predict(X_futuro)\n",
        "\n",
        "df_proj_valores = pd.DataFrame(previsoes_array, columns=targets)\n",
        "fato_2034_proj = pd.concat([meses_faltantes.reset_index(drop=True), df_proj_valores], axis=1)\n",
        "fato_2034_proj['flag_previsao'] = 1\n",
        "\n",
        "# --- C. CONSOLIDAÇÃO E LIMPEZA ---\n",
        "# Pegamos o Real (Até Fevereiro)\n",
        "df_real = df_train[~((df_train['ano'] == 2034) & (df_train['mes'] >= 3))].copy()\n",
        "df_real['flag_previsao'] = 0\n",
        "\n",
        "fato_consolidada = pd.concat([df_real, fato_2034_proj], ignore_index=True)\n",
        "\n",
        "# Reconstrução da sk_tempo (Chave diária para o BI)\n",
        "fato_consolidada['sk_tempo'] = (fato_consolidada['ano'] * 10000 + fato_consolidada['mes'] * 100 + 1).astype(int)\n",
        "\n",
        "# --- D. CÁLCULO DE MÉTRICAS E TIPAGEM ---\n",
        "# Forçamos float para cálculo e depois Decimal/Int para o BQ\n",
        "fato_consolidada['vr_venda'] = fato_consolidada['vr_venda'].astype(float)\n",
        "\n",
        "fato_consolidada['vr_medio_ingresso'] = (fato_consolidada['vr_venda'] / fato_consolidada['qt_ingresso']).replace([np.inf, -np.inf], 0).fillna(0)\n",
        "fato_consolidada['qt_ingresso_por_evento'] = (fato_consolidada['qt_ingresso'] / fato_consolidada['qt_evento']).replace([np.inf, -np.inf], 0).fillna(0)\n",
        "fato_consolidada['qt_evento_por_produtor'] = (fato_consolidada['qt_evento'] / fato_consolidada['qt_produtor']).replace([np.inf, -np.inf], 0).fillna(0)\n",
        "\n",
        "def para_decimal(x):\n",
        "    return Decimal(str(x)).quantize(Decimal('0.00'), rounding=ROUND_HALF_UP)\n",
        "\n",
        "# Aplicando Tipos Finais (NUMERIC e INTEGER)\n",
        "for col in ['vr_venda', 'vr_medio_ingresso']:\n",
        "    fato_consolidada[col] = fato_consolidada[col].apply(para_decimal)\n",
        "\n",
        "cols_int = ['sk_tempo', 'sk_localidade', 'sk_produtor', 'sk_evento', 'qt_ingresso', 'qt_evento', 'qt_produtor', 'qt_ingresso_por_evento', 'qt_evento_por_produtor', 'flag_previsao']\n",
        "for col in cols_int:\n",
        "    fato_consolidada[col] = fato_consolidada[col].round(0).astype(int)\n",
        "\n",
        "# --- E. CARGA NO BIGQUERY ---\n",
        "tabela_final = f\"{dataset_id}.fato_vendas\"\n",
        "import pandas_gbq\n",
        "pandas_gbq.to_gbq(fato_consolidada, tabela_final, project_id=project_id, if_exists='replace')\n",
        "\n",
        "print(f\"Sucesso! Tabela {tabela_final} criada com {len(fato_consolidada)} linhas e granularidade total.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dA90-C9PbBVT",
        "outputId": "02ce2a09-1069-4b93-b1d4-b831c45a7172"
      },
      "id": "dA90-C9PbBVT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo treinado! Score R²: 0.85\n",
            "Gerando combinações de dimensões para o futuro...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 10894.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sucesso! Tabela site-da-laica.sympla.fato_vendas criada com 28552 linhas e granularidade total.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "pipeline_sympla_case"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}